# 一、（我理解的）模型需要满足的条件
## 必须满足的条件
* 使用LLM内部的某种concept tokens
* 加速推理
## 最好满足的条件
* 自回归
* 离散concept tokens

# 二、方案
## 1 纯自回归
用同一个完整的LLM，同时用作原来的compressor, inference module, decompreesor
即
```text
context/prompt ----> LLM --(AR)--> concept tokens ----> 同一个LLM --(AR)--> other concept tokens ----> 同一个LLM --(AR)--> outputs
```

## 2 "CONTINUE token"

* **目标**：在单个 LLM 内部“在线”产生离散 **concept/latent tokens**（额外词表），并用它们驱动普通 tokens 的生成；concept 的写入位置由模型自己决定（COMMIT），从而形成**变长分段**。

* **网络划分**：

  1. **Shallow blocks（浅层）**：对当前普通 token （包括context和新生成的普通tokens）流做增量前向，得到浅层隐藏状态 `h_shallow_t`。
  2. **ConceptHead（控制头）**：接在浅层之上，输出维度为 `V_c + 1`：

     * `V_c` 个 concept id：表示 **COMMIT**（写入一个 concept token）
     * `CONTINUE`：表示不写入 concept，继续生成普通 token
       （用Gumbel-Softmax + straight-through / 采样等实现离散决策）
  3. **Mid blocks（中间层）**：负责concept tokens、的推理，即只有concept tokens参与这里的前向过程
  4. **Deep blocks（深层）**：负责普通 token 的自回归生成（相当于 decompress / 展开）。

* **在线生成机制（类似MTP, i.e. multi-token prediction）**：

  1. Deep以之前生成的concept tokens（concept前缀） 先自回归生成一个普通 token `x_new`（段内展开）。
  2. 把 `x_new` 追加回浅层输入（更新 shallow KV cache），计算 `ConceptHead(h_shallow[new])`。
  3. 若输出 `CONTINUE`：继续让 Deep 自回归生成下一个普通 token（MTP 继续）。
  4. 若输出某个 concept id：触发 **COMMIT**，将该 concept token 追加到 concept 前缀/记忆中，结束当前段（tail 清空），进入下一段继续生成。
  5. 设定段长上限 `M_max` 防止一直 CONTINUE（超限强制 COMMIT）。

* 例子：
```text
Example (one segment):  concept prefix drives token generation, shallow decides CONTINUE vs COMMIT

Legend:
  c*  = concept/latent token (from V_c)
  x*  = normal token
  CONTINUE = no concept written, keep generating normal tokens
  COMMIT   = write a new concept token, end current segment (clear tail)

Initial state:
  concept_prefix = [c1, c2]
  tail = ∅


Step 1: Deep generates a normal token x3
  [c1, c2] ──► Deep ──► x3
                      │
                      ▼
             append x3 to normal stream (tail = [x3])
                      │
                      ▼
             Shallow blocks (incremental) ──► h_shallow(x3)
                      │
                      ▼
             ConceptHead(h_shallow(x3)) ──► CONTINUE
                      │
                      ▼
             (no concept written, keep going)


Step 2: Deep generates next normal token x4
  [c1, c2] + tail[x3] ──► Deep ──► x4
                              │
                              ▼
                     tail = [x3, x4]
                              │
                              ▼
                     Shallow ──► h_shallow(x4)
                              │
                              ▼
                     ConceptHead ──► CONTINUE
                              │
                              ▼
                     (still no concept written)


Step 3: Deep generates next normal token x5
  [c1, c2] + tail[x3,x4] ──► Deep ──► x5
                                   │
                                   ▼
                          tail = [x3, x4, x5]
                                   │
                                   ▼
                          Shallow ──► h_shallow(x5)
                                   │
                                   ▼
                          ConceptHead ──► COMMIT(c3)
                                   │
                                   ▼
               write concept token: concept_prefix = [c1, c2, c3]
               clear tail: tail = ∅
               (segment ends, next segment begins with new concept_prefix)

```

* **核心缺陷**
  * 模型难以学到什么时候要输出`CONTINUE`信号

* **可能可以缓解核心缺陷的训练方法，但很贵，且可能不稳定   （不是很考虑这个方法，因此下面的不太重要）**

  * **Token LM loss**：teacher forcing 训练 deep 的 next-token CE。
  * **Stop/commit 学习 (L_{\text{stop}})**：随机采样若干位置 (t)，用反事实比较估计优势（类似强化学习policy gradient）：

    * 强制 CONTINUE 得到未来 N-token loss (L_K(t))
    * 强制 COMMIT 得到未来 N-token loss (L_C(t))
    * (A(t)=L_K(t)-L_C(t))
    * 用 advantage-weighted BCE 更新 (s_t=P(\text{COMMIT}))：
      (L_{\text{stop}}(t)= -A^+\log s_t - A^-\log(1-s_t))（对 (A) stop-grad）
  * 可加 rate/预算正则控制平均段长/commit 频率。

这套方案的核心就是：**深层负责段内展开（MTP-like），浅层的 ConceptHead 在每个新 token 后做一次“继续/提交概念”的离散控制，从而在模型内部形成在线、变长的 concept token 序列。**

## 3 CONTINUE token + 分数求和 + 阈值 (或者其它能够使决策可微的技巧)
* 为了解决上一个方法的缺陷，从输出`COMMIT`表示一个新的concept token，改为shallow blocks对每个新的普通token输出一个分数，当连续多个token分数之和超过阈值，就生成一个新的concept tokens(加权求和，然后过head得到)。这样“是否生成新concept tokens”的决策就是可微的
* 类似MoE中的 top-p
* 其它方法：
  * 状态增量阈值
  * 压缩分数
  * 熵
  * 注意力
  * 综合

## 4 对应关系固定，实现简单
* 固定每K个普通tokens生成1个concept tokens
* 或者固定每N个普通tokens生成M个concept tokens(M << N)

## 5 仅对attention 压缩
没想好

