## conditions
### must-have
- high efficiency
- autoregressive training/inference
- discrete concept tokens
- minimal structural changes to the base LLM (LoRA + small new modules)

### preferred
- internal concept tokens only (not exposed to user text)
- online compression during decoding

## v1
- Kept as baseline in `train_qwen.py`.

## v2: Online Concept Compression with Concept-Only Middle Blocks

### core design
- Split decoder layers into:
  - shallow blocks: process all normal tokens
  - middle blocks: process concept tokens only
  - deep blocks: process concept prefix + dynamic normal tail
- Add a `ConceptHead` on top of shallow hidden states:
  - output size = `4096 + 1` (`4096` concept ids + `<NULL>`)
  - use Gumbel-Softmax with straight-through estimator
  - `<NULL>` means current normal token is not represented as concept token

### online boundary
- For each time step `t`, define:
  - `tau(t)`: most recent compressed normal token position (`!= <NULL>`)
  - `tail(t) = h_shallow[tau(t)+1 : t]`
- Deep input at step `t`:
  - `Z_t = [concept_prefix(<=t); tail(t)]`

### deep attention rules
- Concept -> Concept: allowed
- Tail -> Concept: allowed
- Tail -> Tail: causal
- Concept -> Tail: disallowed

### training objective (current version)
- Pure next-token cross entropy only:
  - `CE(logits_t, x[t+1])`
- No extra auxiliary losses and no additional training tricks in this version.

### trainable parts
- New modules:
  - `ConceptHead`
  - `ConceptEmbedding`
- Base model:
  - LoRA adapters on attention/MLP linear layers
- Base backbone weights are frozen except LoRA parameters.

### implementation file
- `train_qwen_2.py`


### å®ç°çš„tricks
1. æ§åˆ¶NULL tokensçš„æ•°é‡
2. æ§åˆ¶è¿ç»­çš„NULL tokensçš„æ•°é‡
   
### ä¼˜åŠ¿
1. ç›¸æ¯”äºLLM as compressor and decompressorï¼Œå¯ä»¥æŠŠç”Ÿæˆçš„å†…å®¹ä¹Ÿå‹ç¼©æˆlatent tokensï¼Œè¿™æ ·ä¸concept tokensçš„æ•…äº‹å¯èƒ½èåˆå¾—æ›´å¥½
2. E2E

---


---

### prompt
å¥½ï¼Œé‚£æˆ‘ç›´æ¥ç»™ä½ ä¸€ä»½**â€œæç®€ + ä¼ªä»£ç çº§â€çš„ coding agent prompt**ï¼ŒåŸºæœ¬æ˜¯ä½ è„‘å­é‡Œé‚£å¥—é€»è¾‘çš„**å¯æ‰§è¡Œè§„æ ¼**ï¼Œå°½é‡å‡å°‘è‡ªç”±å‘æŒ¥ç©ºé—´ã€‚

---

#### Ultra-Compact Coding Agent Promptï¼ˆPseudo-code Styleï¼‰

**Task**
Implement training code for an autoregressive LM with **online concept compression** and **concept-only middle Transformer blocks**.

---

##### Core Idea

* Normal tokens are processed by **shallow layers**.
* After shallow layers, a **concept head** compresses some tokens into **concept tokens** using **Gumbel-Softmax + straight-through**.
* **Middle Transformer blocks run ONLY on concept tokens** (main compute saving).
* **Deep Transformer blocks** see:

  * all concept tokens
  * * a dynamic tail of normal tokens (from last compressed token + 1 to current token)
* Model predicts next normal token autoregressively.

---

##### Modules

```text
ShallowTransformer(x[1:T]) -> h_L[1:T]

ConceptHead(h_L[t]):
    logits = linear(h_L[t])
    z = gumbel_softmax(logits, hard=True)
    e = e_hard + (e_soft - stop_grad(e_soft))
    if argmax(z) == <NULL>: return None
    else: return concept_token
```

```text
MiddleTransformer(concept_tokens) -> concept_tokens_updated
# ONLY concept tokens go through these layers
```

```text
DeepTransformer([concept_tokens_updated ; tail_hiddens]) -> logits
```

---

##### Boundary / Tail Definition

```text
Ï„(t) = index of most recent normal token whose concept != <NULL>
tail(t) = h_L[Ï„(t)+1 : t]    # contiguous, variable length
```

---

##### Training Forward (single sequence)

```text
h_L = ShallowTransformer(x[1:T])

concept_tokens = []
compressed_pos = set()

for t in 1..T:
    c = ConceptHead(h_L[t])
    if c != None:
        concept_tokens.append(c)
        compressed_pos.add(t)

concept_tokens_mid = MiddleTransformer(concept_tokens)

for t in 1..T-1:
    Ï„ = max{i <= t | i in compressed_pos} or 0
    tail = h_L[Ï„+1 : t]

    Z_t = concat(concept_tokens_mid_prefix(t), tail)
    logits_t = DeepTransformer(Z_t)
    loss += CE(logits_t, x[t+1])
```

---

##### Attention Rules (DeepTransformer)

```text
Concept -> Concept : allowed (causal or full)
Tail    -> Concept : allowed
Tail    -> Tail    : causal
Concept -> Tail    : disallowed
```

---

##### Key Constraints

* Middle blocks never see normal tokens.
* `<NULL>` means â€œthis token is NOT represented in deep contextâ€.
* Concept tokens must replace information of compressed normal tokens.
* End-to-end backprop through:
  shallow â†’ concept head (ST) â†’ middle â†’ deep.

---

å¦‚æœä½ ä¸‹ä¸€æ­¥è¦**çœŸæ­£å†™ PyTorch ä»£ç **ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ åšä¸‰ä»¶äº‹ä¹‹ä¸€ï¼ˆä½ é€‰ä¸€ä¸ªå°±è¡Œï¼‰ï¼š

1. æŠŠä¸Šé¢ä¼ªä»£ç  **ç¿»æˆæœ€å° PyTorch ç±»ç»“æ„**
2. ä¸“é—¨å†™ **attention mask / padding é€»è¾‘**ï¼ˆè¿™æ˜¯æœ€å®¹æ˜“å†™é”™çš„ï¼‰
3. è®¾è®¡ä¸€ä¸ª **O(T) è€Œä¸æ˜¯ O(TÂ²)** çš„ batching / forward å®ç°æ–¹å¼


## ç©¶ç«Ÿæ€æ ·å˜é•¿ï¼Ÿ
1. ç”¨æ•´ä¸ªLLM çº¯è‡ªå›å½’ 
2. NULL tokens
3. åŠ å’Œ + é˜ˆå€¼
4. å›ºå®šK -> æ¯Nä¸ªæ™®é€štokensç”ŸæˆMä¸ªlatent tokens
5. å…ˆMTPç”Ÿæˆè‹¥å¹²æ™®é€štokensï¼Œmaybeå†ç”¨ä¸€äº›å¯å‘å¼æ–¹æ³•ç»ˆæ­¢ï¼ˆæ¯”å¦‚é«˜ç†µçš„tokensï¼‰

## v3: pure AR
* ä¼˜åŠ¿ï¼š
  * å‚æ•°æ›´å°‘
  * å†…å­˜å ç”¨æ›´å°‘
  * æ›´å¿«ï¼ˆä¸éœ€è¦prefillå¤šæ¬¡ï¼‰

* how to make it more non-trivial?
  * 1. 2d-RoPE Ã— ï¼ˆè¯­ä¹‰æ ¹æœ¬å°±ä¸å¯¹ï¼‰--> éƒ½ä»1å¼€å§‹è®¡æ•°ï¼Œä¸åŒç§ç±»çš„tokensåŠ ä¸€ä¸ªå›ºå®šbias
  * 2. å¤šå±‚æ¬¡concept (å¯ç»“åˆ2d-RoPE)
  * 3. ä»ç›®æ ‡æ–‡æœ¬ X è‡ªåŠ¨æŠ½å–å¯ç›‘ç£ä¿¡å· ğ‘¦(ğ‘‹)ï¼Œè®© Planner å¿…é¡»èƒ½ä» C é¢„æµ‹è¿™äº› y

----

å¥½ï¼Œä¸‹é¢æ˜¯**æŒ‰ä½ æœ€æ–°ä¿®æ”¹**åçš„**ç®€æ´ä¸­è‹±åŒè¯­ pipeline æ€»ç»“**ï¼Œå¯ä»¥ç›´æ¥ä½œä¸ºç»™ codex çš„æç¤ºè¯ä½¿ç”¨ï¼ˆå·²æ»¡è¶³ï¼šä¸åŒ concept ç±»å‹â†’ä¸åŒè¯è¡¨ï¼›ç»Ÿä¸€ RoPEï¼›ä¸æ segment_posï¼‰ã€‚

---
### prompt
#### ä¸­æ–‡ï¼ˆç®€æ´æç¤ºè¯ï¼‰

ç›®æ ‡ï¼šPlannerâ€“Executor ä¸¤é˜¶æ®µ **Concept-first Decoding**ã€‚concept tokens æ˜¯å†…éƒ¨é«˜å±‚è®¡åˆ’ï¼ŒExecutor ä»…ä¾èµ– concept ç”Ÿæˆæ™®é€šæ–‡æœ¬ã€‚

**Pipelineï¼š**

1. **Plannerï¼ˆæ¦‚å¿µè§„åˆ’ï¼‰**

   * è¾“å…¥ï¼šç”¨æˆ·æŒ‡ä»¤/ä¸Šä¸‹æ–‡ (u)
   * è¾“å‡ºï¼šå¤šç§ç±» concept tokensï¼Œæ¯ä¸€ç±»æ¥è‡ª**ç‹¬ç«‹è¯è¡¨**ï¼š
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * æ¯ä¸€ç±» concept ä¸ºå˜é•¿åºåˆ—ï¼Œä»¥ `<EOS>` ç»“æŸã€‚

2. **æ„é€  Executor è¾“å…¥ï¼ˆä»… conceptï¼‰**

   * å°†ä¸åŒç§ç±»çš„ concept ä¸²è”ï¼Œä¸­é—´ç”¨å¯¹åº”çš„åˆ†éš”ç¬¦ï¼š
     `[<BOS>, <SEP_1>, C^{(1)}, <SEP_2>, C^{(2)}, ...]`
   * **ä½ç½®ç¼–ç **ï¼š

     * æ¯ç§ concept token çš„ position index **å„è‡ªä» 1 å¼€å§‹è®¡æ•°**
     * æ‰€æœ‰ token **ç»Ÿä¸€ä½¿ç”¨ RoPE**
   * **Type embedding**ï¼š

     * ä¸ºæ¯ç§ concept ç±»å‹ï¼ˆä»¥åŠå¯¹åº”çš„ `<SEP>`ï¼‰åŠ å…¥ type embedding
   * æœ€ç»ˆè¾“å…¥ embeddingï¼š
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executorï¼ˆå±•å¼€ç”Ÿæˆï¼‰**

   * æ¡ä»¶ç”Ÿæˆï¼š
     [
     x_{1:T} \sim p(x \mid C)
     ]
   * Executor **ä¸è¯»å–æ™®é€š tokens å‰ç¼€ä½œä¸ºæ¡ä»¶ä¸Šä¸‹æ–‡**
   * æ™®é€š token æŒ‰æ ‡å‡†è‡ªå›å½’ç”Ÿæˆï¼Œæ”¯æŒ KV cacheã€‚

---

#### English (concise prompt)

Goal: Two-stage **Plannerâ€“Executor Concept-first Decoding**. Concept tokens are internal high-level plans; the Executor generates text conditioned only on concepts.

**Pipeline:**

1. **Planner (Concept planning)**

   * Input: user instruction/context (u)
   * Output: multiple **typed concept sequences**, each from a **separate vocabulary**:
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * Each concept sequence is variable-length and ends with `<EOS>`.

2. **Build Executor input (concept-only)**

   * Concatenate concept sequences with type-specific separators:
     `[<BOS>, <SEP_1>, C^(1), <SEP_2>, C^(2), ...]`
   * **Positional encoding**:

     * Position indices **restart from 1 for each concept type**
     * **Unified RoPE** is applied to all tokens
   * **Type embedding**:

     * Add a learned type embedding for each concept type (and its separator)
   * Final embedding:
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executor (Expansion / generation)**

   * Generate text as:
     [
     x_{1:T} \sim p(x \mid C)
     ]
   * The Executor conditions **only on concept tokens** (no normal-token prefix as input).
   * Standard autoregressive decoding with KV cache.

## v3.1: speed up training
1. é¢„æµ‹concept tokensæ—¶MTP ï¼ˆæœ‰ç‚¹ç±»ä¼¼äºARç»“åˆqueryï¼‰
2. é¢„æµ‹ä¸åŒtypeçš„concept tokensæ—¶å¹¶è¡Œè·‘
3. zæ¯éš”Mæ­¥æ›´æ–°ä¸€æ¬¡
4. å…ˆç”¨çŸ­æ–‡æœ¬è®­ç¨³ï¼Œåç»­å†ç”¨é•¿æ–‡æœ¬

## ä½ å¯ä»¥åšä»€ä¹ˆå®éªŒ
1. éšæœºå»æ‰ä¸€äº›concept tokensï¼Œçœ‹çœ‹é‡å»ºæ•ˆæœæ˜¯å¦ä¼šå—å¾ˆå¤§å½±å“ï¼Œä»è€Œè¯å®executoræ˜¯çœŸçš„åœ¨ç”¨concept tokens

ä¸‹é¢è¿™äº›æŒ‡æ ‡éƒ½**å¾ˆä¾¿å®œ**ï¼ˆåŸºæœ¬æ¥è‡ªå·²æœ‰ forward çš„ logits / loss / beam ç»“æœï¼‰ï¼Œä½†èƒ½å¾ˆå¥½åœ°åˆ¤æ–­ï¼šE-step æœç´¢æœ‰æ²¡æœ‰è·‘åã€planner æœ‰æ²¡æœ‰å­¦åˆ°ä¸œè¥¿ã€ä»¥åŠä½ è¿™ä¸ª STï¼ˆ(e_{hard}+e_{soft}-\text{stopgrad}(e_{soft}))ï¼‰æœ‰æ²¡æœ‰åœ¨â€œéª—æ¢¯åº¦â€ã€‚

æˆ‘æŒ‰æ¨¡å—ç»™ä¸€å¥—æœ€å®ç”¨çš„ç›‘æ§æ¸…å•ï¼Œå¹¶æ ‡æ³¨â€œæ­£å¸¸/å¼‚å¸¸â€ä¿¡å·ã€‚

---
----
----
----
## A. Executor æ˜¯å¦çœŸçš„åœ¨ç”¨ zï¼ˆæœ€å…³é”®ï¼‰

### 1) æ¡ä»¶äº’ä¿¡æ¯ä»£ç†ï¼šÎ”NLLï¼ˆablation å·®ï¼‰

æ¯éš”ä¸€æ®µï¼ˆæ¯”å¦‚æ¯ 500ï½2000 stepï¼‰ï¼Œåœ¨ä¸€ä¸ªå¾ˆå°çš„ mini-batch ä¸Šåšä¸¤æ¬¡ teacher forcingï¼š

* æ­£å¸¸ï¼šç”¨å½“å‰é€‰çš„ (z^*) è®¡ç®— ( \text{NLL}(y\mid z^*))
* ç ´å zï¼šæŠŠ z æ¢æˆ

  * åŒ batch é‡Œåˆ«çš„æ ·æœ¬çš„ zï¼ˆshuffle zï¼‰ï¼Œæˆ–
  * å…¨ <MASK>/<PAD>ï¼Œæˆ–
  * éšæœº zï¼ˆåŒé•¿åº¦ï¼‰

è®°ï¼š
[
\Delta = \text{NLL}(y\mid z_{\text{bad}})-\text{NLL}(y\mid z^*)
]

**æ­£å¸¸**ï¼šÎ” > 0 ä¸”é€æ­¥å˜å¤§/ç¨³å®šï¼ˆè¯´æ˜ z æœ‰ä¿¡æ¯ï¼‰
**å¼‚å¸¸**ï¼šÎ”â‰ˆ0ï¼ˆexecutor å¿½ç•¥ zï¼›posterior collapse é£é™©ï¼‰

> è¿™ä¸æ˜¯â€œå¤§é‡è®¡ç®—â€ï¼šåªæ˜¯åœ¨å° batch ä¸Šå¤šè·‘ä¸€æ¬¡ forwardã€‚

---

## B. E-step æœç´¢è´¨é‡ä¸ç¨³å®šæ€§ï¼ˆHard-EM æ˜¯å¦å¥åº·ï¼‰

### 2) Top-1 marginï¼ˆèƒœå‡ºå¹…åº¦ï¼‰

ä½ åœ¨ E-step ä¼šå¾—åˆ° top-k å€™é€‰çš„è¯„åˆ† (S(z)=\log p_\theta(y|z)+\log p_\phi(z|u))ã€‚

ç›‘æ§ï¼š
[
m = S(z^{(1)})-S(z^{(2)})
]

**æ­£å¸¸**ï¼šm é€‚ä¸­ï¼ˆæœ‰åŒºåˆ†åº¦ï¼Œä½†ä¸æ€»æ˜¯å·¨å¤§ï¼‰
**å¼‚å¸¸1**ï¼šmâ‰ˆ0 é•¿æœŸï¼ˆæœç´¢ä¸åˆ†èƒœè´Ÿï¼Œå™ªå£°å¤§ï¼Œz* ä¸ç¨³å®šï¼‰
**å¼‚å¸¸2**ï¼šm å·¨å¤§ä¸”å¾ˆæ—©å‡ºç°ï¼ˆè¿‡æ—©å¡Œç¼©åˆ°å›ºå®šæ¨¡å¼/ä½å¤šæ ·æ€§ï¼‰

### 3) z* å˜æ›´ç‡ï¼ˆassignment flip rateï¼‰

æ¯æ¬¡ä½ æ›´æ–° E-step æ—¶ï¼Œç»Ÿè®¡ï¼š

* æœ‰å¤šå°‘æ ·æœ¬çš„ (z^*) è·Ÿä¸Šæ¬¡ä¸åŒï¼ˆæˆ–ç¼–è¾‘è·ç¦»>é˜ˆå€¼ï¼‰

**æ­£å¸¸**ï¼šå‰æœŸé«˜ã€åæœŸä¸‹é™
**å¼‚å¸¸**ï¼šä¸€ç›´å¾ˆé«˜ï¼ˆæ¨¡å‹åœ¨éœ‡è¡/å­¦ä¹ ç‡è¿‡å¤§/è¯„åˆ†ä¸å¯é ï¼‰
**å¼‚å¸¸**ï¼šå¾ˆæ—©é™åˆ°æ¥è¿‘ 0ï¼ˆè¿‡æ—©é”æ­»ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰

> è¿™ä¸ªå‡ ä¹é›¶æˆæœ¬ï¼šä½ æœ¬æ¥å°±æœ‰ z*ï¼Œåªè¦åšä¸ª hash/ç¼–è¾‘è·ç¦»ã€‚

### 4) â€œæ—§ z* ä»å¥½ä¸å¥½ç”¨â€ï¼ˆstaleness gapï¼‰

åœ¨ç¨€ç–æ›´æ–° E-step æ—¶ï¼š
åŒä¸€ä¸ªæ ·æœ¬ï¼Œç”¨æ—§ (z^**{\text{old}}) å’Œæ–° (z^**{\text{new}}) çš„åˆ†æ•°å·®ï¼š

[
g = S(z^**{\text{new}})-S(z^**{\text{old}})
]

**æ­£å¸¸**ï¼šg å°ä¸”é€æ­¥å˜å°ï¼ˆè¯´æ˜ç¨€ç–æ›´æ–°æ²¡ä¼¤å¤ªå¤šï¼‰
**å¼‚å¸¸**ï¼šg å¸¸å¸¸å¾ˆå¤§ï¼ˆè¯´æ˜ä½  stale å¤ªä¹…ï¼Œåº”è¯¥æ›´é¢‘ç¹æ›´æ–°æˆ–å‡å°LRï¼‰

---

## C. Planner æ˜¯å¦åœ¨å­¦ï¼ˆä»¥åŠ ST æ˜¯å¦â€œå¯¹é½â€ï¼‰

### 5) Planner ç†µï¼ˆæˆ– top-1 æ¦‚ç‡å‡å€¼ï¼‰

ä» planner çš„ soft åˆ†å¸ƒé‡Œå–ä¸€ä¸ªä¾¿å®œç»Ÿè®¡ï¼š

* æ¯ä¸ªä½ç½®çš„ entropy (H(p_t)) çš„å‡å€¼
* æˆ– mean max-probï¼š(\mathbb{E}[\max_i p_t(i)])

**æ­£å¸¸**ï¼šå‰æœŸé«˜ç†µâ†’ä¸­æœŸé€æ­¥å˜å°–â†’åæœŸç¨³å®š
**å¼‚å¸¸1**ï¼šä¸€ç›´é«˜ç†µï¼ˆplanner æ²¡å­¦åˆ°å¯ç”¨ latentï¼‰
**å¼‚å¸¸2**ï¼šå¾ˆå¿«æä½ç†µï¼ˆè¿‡æ—©ç¡®å®šã€å€™é€‰å¤šæ ·æ€§ä¸è¶³ã€Hard-EM æ˜“å¡æ­»ï¼‰

### 6) e_soft ä¸ e_hard çš„ä¸€è‡´æ€§ï¼ˆcosine / L2ï¼‰

ä½ æœ¬æ¥å°±ç®—äº† (e_{soft}) å’Œ (e_{hard})ã€‚ç›‘æ§ï¼š

* (\cos(e_{soft}, e_{hard})) çš„å‡å€¼ï¼ˆæŒ‰ token æˆ–æŒ‰åºåˆ—å¹³å‡ï¼‰
* æˆ– (|e_{soft}-e_{hard}|)

**æ­£å¸¸**ï¼šéšè®­ç»ƒé€æ­¥æ›´ä¸€è‡´ï¼ˆsoft æ›´æ¥è¿‘ one-hotï¼‰
**å¼‚å¸¸**ï¼šé•¿æœŸä¸ä¸€è‡´ï¼ˆST æ¢¯åº¦åœ¨ä¼˜åŒ–â€œæ¨¡ç³Š embeddingâ€ï¼Œforward å´èµ°ç¡¬ tokenï¼Œå®¹æ˜“ driftï¼‰

> è¿™ä¸ªä¹Ÿå‡ ä¹é›¶æˆæœ¬ï¼Œå› ä¸º embedding å·²ç»åœ¨ forward é‡Œäº†ã€‚

### 7) Planner æ¢¯åº¦èŒƒæ•°ï¼ˆå…¨å±€æˆ–æœ€åä¸€å±‚ï¼‰

è®°å½•ä¸€ä¸ªç®€å•æ ‡é‡ï¼š

* (|\nabla_\phi|)ï¼ˆglobal grad normï¼‰
* æˆ–æŸå±‚ grad norm

**æ­£å¸¸**ï¼šæœ‰ä¿¡å·ã€ä¸è¿‡åº¦çˆ†ç‚¸
**å¼‚å¸¸**ï¼šæ¥è¿‘ 0ï¼ˆexecutor ä¸ä¾èµ– z / ST æ— æœ‰æ•ˆä¿¡å·ï¼‰
**å¼‚å¸¸**ï¼šé¢‘ç¹çˆ†ç‚¸ï¼ˆæ¸©åº¦è¿‡ä½ã€å­¦ä¹ ç‡è¿‡å¤§ã€è¯„åˆ†éœ‡è¡ï¼‰

---

## D. è¾“å‡ºä¾§ï¼šè®­ç»ƒæ˜¯å¦åœ¨â€œé•¿åº¦æ‰©å±•â€æ—¶å´©

### 8) åˆ†æ®µ NLLï¼ˆæŒ‰ä½ç½®æ¡¶ï¼‰

ä¸éœ€è¦é¢å¤– forwardï¼šä½  teacher forcing æœ¬æ¥å°±ç®—äº†æ¯ä¸ª token çš„ lossã€‚
æŠŠ loss æŒ‰ä½ç½®åˆ†æ¡¶ï¼ˆæ¯”å¦‚ 0-64, 64-128, â€¦ï¼‰æ±‚å‡å€¼ã€‚

**æ­£å¸¸**ï¼šå„æ¡¶éƒ½é€æ­¥ä¸‹é™ï¼›æ‰©å±•é•¿åº¦æ—¶åæ®µå…ˆæŠ–å†ç¨³
**å¼‚å¸¸**ï¼šåªå‰ 64 é™ï¼Œåé¢ä¸€ç›´ä¸åŠ¨ï¼ˆz å¯èƒ½åªæœåŠ¡å¼€å¤´ / executor é•¿ç¨‹æ²¡å­¦åˆ°ï¼‰

---

# ä¸€å¥—æœ€å°ä½†å¤Ÿç”¨çš„ç›‘æ§ç»„åˆï¼ˆå»ºè®®ä½ å°±ç”¨è¿™ 6 ä¸ªï¼‰

1. **Î”NLL(z-shuffle)**ï¼šexecutor æ˜¯å¦ç”¨ z
2. **Top-1 margin**ï¼šE-step åŒºåˆ†åº¦
3. **z* flip rate**ï¼šç¨³å®šæ€§/æ˜¯å¦éœ‡è¡
4. **staleness gap**ï¼šç¨€ç–æ›´æ–°æ˜¯å¦è¿‡å¤´
5. **planner entropy / max-prob**ï¼šæ˜¯å¦å¡Œç¼©æˆ–æ²¡å­¦
6. **cos(e_soft, e_hard)**ï¼šST å¯¹é½æ˜¯å¦å¥åº·

è¿™äº›å‡ ä¹éƒ½ä¸éœ€è¦é¢å¤–å¤§è®¡ç®—ï¼Œåªæœ‰ç¬¬1ä¸ªéœ€è¦å¶å°”å¤šä¸€æ¬¡ forwardï¼ˆå° batch å³å¯ï¼‰ã€‚

---

## å¿«é€Ÿâ€œå¼‚å¸¸â†’åŠ¨ä½œâ€å¯¹ç…§è¡¨

* Î”NLLâ‰ˆ0ï¼šåŠ å¼º bottleneckï¼ˆexecutor ä¸çœ‹ uï¼‰ã€å¢å¤§ z å®¹é‡/è¯è¡¨ã€é™ä½ executor å­¦ä¹ ç‡ã€æé«˜ z dropout/æ‰°åŠ¨å¯¹æ¯”
* marginâ‰ˆ0 ä¸” flip é«˜ï¼šæé«˜å€™é€‰ K æˆ–å¤šæ ·æ€§ï¼›E-step æ›´é¢‘ç¹ï¼›é™ä½å­¦ä¹ ç‡
* flip å¾ˆæ—©å˜ 0 + ç†µå¾ˆä½ï¼šæé«˜æ¸©åº¦/ç†µæ­£åˆ™ï¼›ç”¨ diverse beamï¼›å¶å°”å¼ºåˆ¶æ¢ç´¢ï¼ˆé‡‡æ ·å€™é€‰ï¼‰
* staleness gap å¤§ï¼šç¼©çŸ­ E-step æ›´æ–°é—´éš”æˆ–ç”¨è‡ªé€‚åº”æ›´æ–°ï¼›å‡ LR
* cos(e_soft,e_hard) é•¿æœŸä½ï¼šé™æ¸©åº¦æˆ– annealï¼›å¯¹ soft åŠ  sharpenï¼ˆä½†åˆ«ä¸€ä¸‹å­å¤ªç¡¬ï¼‰
