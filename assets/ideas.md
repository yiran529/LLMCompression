## conditions
### must-have
- high efficiency
- autoregressive training/inference
- discrete concept tokens
- minimal structural changes to the base LLM (LoRA + small new modules)

### preferred
- internal concept tokens only (not exposed to user text)
- online compression during decoding

## v1
- Kept as baseline in `train_qwen.py`.

## v2: Online Concept Compression with Concept-Only Middle Blocks

### core design
- Split decoder layers into:
  - shallow blocks: process all normal tokens
  - middle blocks: process concept tokens only
  - deep blocks: process concept prefix + dynamic normal tail
- Add a `ConceptHead` on top of shallow hidden states:
  - output size = `4096 + 1` (`4096` concept ids + `<NULL>`)
  - use Gumbel-Softmax with straight-through estimator
  - `<NULL>` means current normal token is not represented as concept token

### online boundary
- For each time step `t`, define:
  - `tau(t)`: most recent compressed normal token position (`!= <NULL>`)
  - `tail(t) = h_shallow[tau(t)+1 : t]`
- Deep input at step `t`:
  - `Z_t = [concept_prefix(<=t); tail(t)]`

### deep attention rules
- Concept -> Concept: allowed
- Tail -> Concept: allowed
- Tail -> Tail: causal
- Concept -> Tail: disallowed

### training objective (current version)
- Pure next-token cross entropy only:
  - `CE(logits_t, x[t+1])`
- No extra auxiliary losses and no additional training tricks in this version.

### trainable parts
- New modules:
  - `ConceptHead`
  - `ConceptEmbedding`
- Base model:
  - LoRA adapters on attention/MLP linear layers
- Base backbone weights are frozen except LoRA parameters.

### implementation file
- `train_qwen_2.py`


### å®ç°çš„tricks
1. æ§åˆ¶NULL tokensçš„æ•°é‡
2. æ§åˆ¶è¿ç»­çš„NULL tokensçš„æ•°é‡
   
### ä¼˜åŠ¿
1. ç›¸æ¯”äºLLM as compressor and decompressorï¼Œå¯ä»¥æŠŠç”Ÿæˆçš„å†…å®¹ä¹Ÿå‹ç¼©æˆlatent tokensï¼Œè¿™æ ·ä¸concept tokensçš„æ•…äº‹å¯èƒ½èåˆå¾—æ›´å¥½
2. E2E

---


---

### prompt
å¥½ï¼Œé‚£æˆ‘ç›´æ¥ç»™ä½ ä¸€ä»½**â€œæç®€ + ä¼ªä»£ç çº§â€çš„ coding agent prompt**ï¼ŒåŸºæœ¬æ˜¯ä½ è„‘å­é‡Œé‚£å¥—é€»è¾‘çš„**å¯æ‰§è¡Œè§„æ ¼**ï¼Œå°½é‡å‡å°‘è‡ªç”±å‘æŒ¥ç©ºé—´ã€‚

---

#### Ultra-Compact Coding Agent Promptï¼ˆPseudo-code Styleï¼‰

**Task**
Implement training code for an autoregressive LM with **online concept compression** and **concept-only middle Transformer blocks**.

---

##### Core Idea

* Normal tokens are processed by **shallow layers**.
* After shallow layers, a **concept head** compresses some tokens into **concept tokens** using **Gumbel-Softmax + straight-through**.
* **Middle Transformer blocks run ONLY on concept tokens** (main compute saving).
* **Deep Transformer blocks** see:

  * all concept tokens
  * * a dynamic tail of normal tokens (from last compressed token + 1 to current token)
* Model predicts next normal token autoregressively.

---

##### Modules

```text
ShallowTransformer(x[1:T]) -> h_L[1:T]

ConceptHead(h_L[t]):
    logits = linear(h_L[t])
    z = gumbel_softmax(logits, hard=True)
    e = e_hard + (e_soft - stop_grad(e_soft))
    if argmax(z) == <NULL>: return None
    else: return concept_token
```

```text
MiddleTransformer(concept_tokens) -> concept_tokens_updated
# ONLY concept tokens go through these layers
```

```text
DeepTransformer([concept_tokens_updated ; tail_hiddens]) -> logits
```

---

##### Boundary / Tail Definition

```text
Ï„(t) = index of most recent normal token whose concept != <NULL>
tail(t) = h_L[Ï„(t)+1 : t]    # contiguous, variable length
```

---

##### Training Forward (single sequence)

```text
h_L = ShallowTransformer(x[1:T])

concept_tokens = []
compressed_pos = set()

for t in 1..T:
    c = ConceptHead(h_L[t])
    if c != None:
        concept_tokens.append(c)
        compressed_pos.add(t)

concept_tokens_mid = MiddleTransformer(concept_tokens)

for t in 1..T-1:
    Ï„ = max{i <= t | i in compressed_pos} or 0
    tail = h_L[Ï„+1 : t]

    Z_t = concat(concept_tokens_mid_prefix(t), tail)
    logits_t = DeepTransformer(Z_t)
    loss += CE(logits_t, x[t+1])
```

---

##### Attention Rules (DeepTransformer)

```text
Concept -> Concept : allowed (causal or full)
Tail    -> Concept : allowed
Tail    -> Tail    : causal
Concept -> Tail    : disallowed
```

---

##### Key Constraints

* Middle blocks never see normal tokens.
* `<NULL>` means â€œthis token is NOT represented in deep contextâ€.
* Concept tokens must replace information of compressed normal tokens.
* End-to-end backprop through:
  shallow â†’ concept head (ST) â†’ middle â†’ deep.

---

å¦‚æœä½ ä¸‹ä¸€æ­¥è¦**çœŸæ­£å†™ PyTorch ä»£ç **ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ åšä¸‰ä»¶äº‹ä¹‹ä¸€ï¼ˆä½ é€‰ä¸€ä¸ªå°±è¡Œï¼‰ï¼š

1. æŠŠä¸Šé¢ä¼ªä»£ç  **ç¿»æˆæœ€å° PyTorch ç±»ç»“æ„**
2. ä¸“é—¨å†™ **attention mask / padding é€»è¾‘**ï¼ˆè¿™æ˜¯æœ€å®¹æ˜“å†™é”™çš„ï¼‰
3. è®¾è®¡ä¸€ä¸ª **O(T) è€Œä¸æ˜¯ O(TÂ²)** çš„ batching / forward å®ç°æ–¹å¼


## ç©¶ç«Ÿæ€æ ·å˜é•¿ï¼Ÿ
1. ç”¨æ•´ä¸ªLLM çº¯è‡ªå›å½’ 
2. NULL tokens
3. åŠ å’Œ + é˜ˆå€¼
4. å›ºå®šK -> æ¯Nä¸ªæ™®é€štokensç”ŸæˆMä¸ªlatent tokens
5. å…ˆMTPç”Ÿæˆè‹¥å¹²æ™®é€štokensï¼Œmaybeå†ç”¨ä¸€äº›å¯å‘å¼æ–¹æ³•ç»ˆæ­¢ï¼ˆæ¯”å¦‚é«˜ç†µçš„tokensï¼‰

## v3: pure AR
* ä¼˜åŠ¿ï¼š
  * å‚æ•°æ›´å°‘
  * å†…å­˜å ç”¨æ›´å°‘
  * æ›´å¿«ï¼ˆä¸éœ€è¦prefillå¤šæ¬¡ï¼‰

* how to make it more non-trivial?
  * 1. 2d-RoPE Ã— ï¼ˆè¯­ä¹‰æ ¹æœ¬å°±ä¸å¯¹ï¼‰--> éƒ½ä»1å¼€å§‹è®¡æ•°ï¼Œä¸åŒç§ç±»çš„tokensåŠ ä¸€ä¸ªå›ºå®šbias
  * 2. å¤šå±‚æ¬¡concept (å¯ç»“åˆ2d-RoPE)
  * 3. ä»ç›®æ ‡æ–‡æœ¬ X è‡ªåŠ¨æŠ½å–å¯ç›‘ç£ä¿¡å· ğ‘¦(ğ‘‹)ï¼Œè®© Planner å¿…é¡»èƒ½ä» C é¢„æµ‹è¿™äº› y

----

å¥½ï¼Œä¸‹é¢æ˜¯**æŒ‰ä½ æœ€æ–°ä¿®æ”¹**åçš„**ç®€æ´ä¸­è‹±åŒè¯­ pipeline æ€»ç»“**ï¼Œå¯ä»¥ç›´æ¥ä½œä¸ºç»™ codex çš„æç¤ºè¯ä½¿ç”¨ï¼ˆå·²æ»¡è¶³ï¼šä¸åŒ concept ç±»å‹â†’ä¸åŒè¯è¡¨ï¼›ç»Ÿä¸€ RoPEï¼›ä¸æ segment_posï¼‰ã€‚

---
### prompt
#### ä¸­æ–‡ï¼ˆç®€æ´æç¤ºè¯ï¼‰

ç›®æ ‡ï¼šPlannerâ€“Executor ä¸¤é˜¶æ®µ **Concept-first Decoding**ã€‚concept tokens æ˜¯å†…éƒ¨é«˜å±‚è®¡åˆ’ï¼ŒExecutor ä»…ä¾èµ– concept ç”Ÿæˆæ™®é€šæ–‡æœ¬ã€‚

**Pipelineï¼š**

1. **Plannerï¼ˆæ¦‚å¿µè§„åˆ’ï¼‰**

   * è¾“å…¥ï¼šç”¨æˆ·æŒ‡ä»¤/ä¸Šä¸‹æ–‡ (u)
   * è¾“å‡ºï¼šå¤šç§ç±» concept tokensï¼Œæ¯ä¸€ç±»æ¥è‡ª**ç‹¬ç«‹è¯è¡¨**ï¼š
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * æ¯ä¸€ç±» concept ä¸ºå˜é•¿åºåˆ—ï¼Œä»¥ `<EOS>` ç»“æŸã€‚

2. **æ„é€  Executor è¾“å…¥ï¼ˆä»… conceptï¼‰**

   * å°†ä¸åŒç§ç±»çš„ concept ä¸²è”ï¼Œä¸­é—´ç”¨å¯¹åº”çš„åˆ†éš”ç¬¦ï¼š
     `[<BOS>, <SEP_1>, C^{(1)}, <SEP_2>, C^{(2)}, ...]`
   * **ä½ç½®ç¼–ç **ï¼š

     * æ¯ç§ concept token çš„ position index **å„è‡ªä» 1 å¼€å§‹è®¡æ•°**
     * æ‰€æœ‰ token **ç»Ÿä¸€ä½¿ç”¨ RoPE**
   * **Type embedding**ï¼š

     * ä¸ºæ¯ç§ concept ç±»å‹ï¼ˆä»¥åŠå¯¹åº”çš„ `<SEP>`ï¼‰åŠ å…¥ type embedding
   * æœ€ç»ˆè¾“å…¥ embeddingï¼š
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executorï¼ˆå±•å¼€ç”Ÿæˆï¼‰**

   * æ¡ä»¶ç”Ÿæˆï¼š
     x_t ~ p(x_tâ€‹âˆ£C,x_{ < t} â€‹)
   * Executor **ä¸è¯»å–æ™®é€š tokens è¾“å…¥å‰ç¼€ä½œä¸ºæ¡ä»¶ä¸Šä¸‹æ–‡**
   * æ™®é€š token æŒ‰æ ‡å‡†è‡ªå›å½’ç”Ÿæˆï¼Œæ”¯æŒ KV cacheã€‚
   * ä¼šæŠŠå·²ç”Ÿæˆæ™®é€š tokens ä½œä¸ºâ€œæ¡ä»¶ä¸Šä¸‹æ–‡â€å†è¾“å…¥æ¨¡å‹

---

#### English (concise prompt)

Goal: Two-stage **Plannerâ€“Executor Concept-first Decoding**. Concept tokens are internal high-level plans; the Executor generates text conditioned only on concepts.

**Pipeline:**

1. **Planner (Concept planning)**

   * Input: user instruction/context (u)
   * Output: multiple **typed concept sequences**, each from a **separate vocabulary**:
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * Each concept sequence is variable-length and ends with `<EOS>`.

2. **Build Executor input (concept-only)**

   * Concatenate concept sequences with type-specific separators:
     `[<BOS>, <SEP_1>, C^(1), <SEP_2>, C^(2), ...]`
   * **Positional encoding**:

     * Position indices **restart from 1 for each concept type**
     * **Unified RoPE** is applied to all tokens
   * **Type embedding**:

     * Add a learned type embedding for each concept type (and its separator)
   * Final embedding:
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executor (Expansion / generation)**

   * Generate text as:
     [
     x_{1:T} \sim p(x \mid C)
     ]
   * The Executor conditions **only on concept tokens** (no normal-token prefix as input).
   * Standard autoregressive decoding with KV cache.
  
### ä¸Šä¸‹æ–‡
#### A
æˆ‘æå‡ºäº†ä¸€ä¸ªLLMæ¨ç†åŠ é€Ÿçš„æ–¹æ¡ˆï¼šå…ˆç”¨LLMå°†è¾“å…¥è‡ªå›å½’å‹ç¼©åˆ°å°‘é‡çš„ç¦»æ•£concept tokensç»„æˆçš„åºåˆ—ä¸­ï¼ˆæ¥è‡ªä¸€ä¸ªæ–°çš„è¯è¡¨ï¼‰ï¼Œå†ç”¨åŒä¸€ä¸ªLLMå°†concept tokensåºåˆ—è‡ªå›å½’ç”Ÿæˆè¾“å‡ºã€‚ç®€è¦æ€»ç»“æˆ‘è¿™ä¸ªæ–¹æ³•ï¼Œç¡®ä¿ä½ ç†è§£äº†æˆ‘çš„æ–¹æ¡ˆçš„ç»†èŠ‚ï¼Œåç»­å¯¹è¯å°†åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œã€‚ä¸€äº›æ›´å…·ä½“çš„ç»†èŠ‚å¦‚ä¸‹: â€¦â€¦ï¼ˆå°†å‰é¢çš„ä¸­æ–‡promptç²˜è´´ä¸Šæ¥ï¼‰
#### B
é€šè¿‡åˆ†æä½ æä¾›çš„ä»£ç ï¼ˆ`model.py`ã€`train.py`ã€`infer.py` ç­‰ï¼‰ï¼Œæˆ‘ä¸ºä½ æ€»ç»“äº†è¿™ä¸ªé¡¹ç›®çš„æ¨¡å‹æ¶æ„ä¸è®­ç»ƒæµç¨‹ã€‚è¿™æ˜¯ä¸€ä¸ª**åŸºäºâ€œæ¦‚å¿µè§„åˆ’ï¼ˆPlannerï¼‰- æ‰§è¡Œï¼ˆExecutorï¼‰â€ä¸¤é˜¶æ®µç”Ÿæˆæ¡†æ¶**çš„ Token å‹ç¼©/å—æ§ç”Ÿæˆæ¨¡å‹ã€‚

##### 1. æ ¸å¿ƒæ¨¡å‹æ¶æ„ (`SharedBackboneUnifiedHead`)

è¯¥æ¨¡å‹æ—¨åœ¨é€šè¿‡**å…±äº«éª¨å¹²ç½‘ç»œ**ï¼ˆShared Backboneï¼‰åŒæ—¶å®Œæˆâ€œæ¦‚å¿µè§„åˆ’â€å’Œâ€œæ–‡æœ¬ç”Ÿæˆâ€ä¸¤ä¸ªä»»åŠ¡ã€‚

*   **åŸºç¡€éª¨å¹² (Backbone)**:
    *   ä½¿ç”¨ä¸€ä¸ªé¢„è®­ç»ƒçš„å› æœè¯­è¨€æ¨¡å‹ï¼ˆ`AutoModelForCausalLM`ï¼‰ä½œä¸ºç‰¹å¾æå–å™¨ã€‚
    *   é›†æˆäº† **LoRA (Low-Rank Adaptation)** è¿›è¡Œå‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œå¤§éƒ¨åˆ†åŸºç¡€å‚æ•°è¢«å†»ç»“ï¼Œåªè®­ç»ƒ LoRA æƒé‡ã€‚
*   **ç»Ÿä¸€åµŒå…¥å±‚ (Unified Embeddings)**:
    *   **Token Embedding**: åˆ†ä¸º `base`ï¼ˆåŸæ¨¡å‹è¯è¡¨ï¼‰å’Œ `new`ï¼ˆæ–°å¢çš„æ¦‚å¿µ Tokenï¼‰ã€‚
    *   **Type Embedding**: å¼•å…¥äº† `type_embed`ï¼Œç”¨äºåŒºåˆ†å½“å‰æ˜¯åœ¨å¤„ç†æ™®é€šæ–‡æœ¬ã€è¿˜æ˜¯æŸç§ç‰¹å®šç±»å‹çš„æ¦‚å¿µï¼ˆå¦‚ `ConceptTypeConfig` å®šä¹‰çš„å¤šç§ç±»å‹ï¼‰ã€‚
    *   è¾“å…¥æœ€ç»ˆæ˜¯ `Token Embedding + Type Embedding`ã€‚
*   **ç»Ÿä¸€è¾“å‡ºå¤´ (Unified Output Head)**:
    *   **Base Head**: æ˜ å°„å›åŸè¯è¡¨å¤§å°ï¼Œç”¨äºç”Ÿæˆæ™®é€šæ–‡æœ¬ã€‚
    *   **New Head**: æ˜ å°„åˆ°æ–°å¢çš„æ¦‚å¿µè¯è¡¨ï¼Œç”¨äºç”Ÿæˆæ¦‚å¿µ Tokenã€‚
    *   å‰å‘ä¼ æ’­æ—¶ï¼Œæ ¹æ®å½“å‰ä»»åŠ¡è¾“å‡ºæ‹¼æ¥åçš„ Logitsã€‚
*   **é…é¢æ§åˆ¶å™¨ (PlannerQuotaController)**:
    *   ç”¨äºåœ¨è§„åˆ’é˜¶æ®µåŠ¨æ€è°ƒèŠ‚ç”Ÿæˆçš„â€œé…é¢â€ï¼ˆQuotaï¼‰ï¼Œæ§åˆ¶ç”Ÿæˆæ•°é‡æˆ–åˆ†å¸ƒã€‚

##### 2. è®­ç»ƒæµç¨‹ (ä¸¤é˜¶æ®µè®­ç»ƒ)

è®­ç»ƒåœ¨ä¸€ä¸ª Step ä¸­åˆ†ä¸¤ä¸ªé˜¶æ®µè¿›è¡Œï¼Œä¸¤ä¸ªé˜¶æ®µå…±äº«æ¢¯åº¦æ›´æ–°ã€‚

###### ç¬¬ä¸€é˜¶æ®µï¼šè§„åˆ’å™¨ (Planner Stage)
**ç›®æ ‡**ï¼šæ ¹æ®è¾“å…¥æ–‡æœ¬ï¼Œç”Ÿæˆä¸€ç³»åˆ—é«˜å±‚çš„â€œæ¦‚å¿µâ€Tokenã€‚è¿™æ˜¯ä¸€ä¸ª**è‡ªå›å½’ç”Ÿæˆçš„**è¿‡ç¨‹ï¼Œä½†åœ¨è®­ç»ƒæ—¶æ˜¯åœ¨çº¿ç”Ÿæˆçš„ã€‚

1.  **æ„å»º Prompt**: `[BOS, input_ids, <PLAN>]`ã€‚æ¨¡å‹çœ‹åˆ° `<PLAN>` åå¼€å§‹ç”Ÿæˆæ¦‚å¿µã€‚
2.  **å¼‚æ­¥æ¦‚å¿µç”Ÿæˆ (`plan_concepts`)**:
    *   æ¨¡å‹å¹¶è¡Œåœ°ä¸ºå®šä¹‰å¥½çš„å¤šç§ `ConceptType` ç”Ÿæˆåºåˆ—ã€‚
    *   **æœºåˆ¶**ï¼šä½¿ç”¨ Gumbel-Softmax è¿›è¡Œ**å¯å¾®é‡‡æ ·**ï¼ˆSoft/Hard ç»“åˆï¼‰ï¼Œä½¿å¾—åç»­çš„æ‰§è¡Œå™¨å¯ä»¥é€šè¿‡æ¢¯åº¦å›ä¼ ä¼˜åŒ–è§„åˆ’å™¨ã€‚
    *   **çº¦æŸ**ï¼š
        *   `commitment_loss`: çº¦æŸè½¯é‡‡æ ·ï¼ˆGumbel Softmax è¾“å‡ºï¼‰å’Œç¡¬é‡‡æ ·ï¼ˆOne-hotï¼‰çš„ä¸€è‡´æ€§ã€‚
        *   `usage_kl_to_uniform`: é˜²æ­¢æ¦‚å¿µåç¼©ï¼ˆConcept Collapseï¼‰ï¼Œé¼“åŠ±å¤šæ ·æ€§ã€‚
        *   `loss_eos`: é¼“åŠ±æ¨¡å‹é€‚æ—¶ç”Ÿæˆ EOSï¼Œæ§åˆ¶æ¦‚å¿µåºåˆ—é•¿åº¦ã€‚
        *   `loss_quota`: é€šè¿‡é…é¢æ§åˆ¶å™¨çº¦æŸç”Ÿæˆçš„æ€»é‡ã€‚
3.  **è¾“å‡º**: å¾—åˆ°æ¯ä¸ªç±»å‹çš„æ¦‚å¿µåºåˆ—ï¼ˆHard ID ç”¨äºæ„å»º Promptï¼ŒSoft Embeddings ç”¨äºå¯èƒ½çš„æ¢¯åº¦æµæˆ–åˆ†æï¼‰ã€‚

###### ç¬¬äºŒé˜¶æ®µï¼šæ‰§è¡Œå™¨ (Executor / BPTT Stage)
**ç›®æ ‡**ï¼šåŸºäºè§„åˆ’å™¨ç”Ÿæˆçš„æ¦‚å¿µåºåˆ—ä½œä¸ºå‰ç¼€ï¼ˆPrefixï¼‰ï¼Œé‡å»º/ç”ŸæˆåŸå§‹æ–‡æœ¬ã€‚

1.  **æ„å»ºæ‰§è¡Œå‰ç¼€ (`build_executor_prefix`)**:
    *   å°†ç¬¬ä¸€é˜¶æ®µç”Ÿæˆçš„æ¦‚å¿µåºåˆ—ï¼ˆåŒ…æ‹¬ Type IDã€Position ID ç­‰ï¼‰æ‹¼æ¥èµ·æ¥ã€‚
    *   ç»“æ„ï¼š`[Concept_Type_1..., Concept_Type_2..., BOS]`ã€‚
2.  **å‰å‘ä¼ æ’­ä¸è®¡ç®— Loss**:
    *   **è¾“å…¥**: `[Executor Prefix, BOS, input_ids]`
    *   **ç›®æ ‡**: é¢„æµ‹ `input_ids`ï¼ˆNext Token Predictionï¼‰ã€‚
    *   **Mask**: æ‰§è¡Œå™¨é€šè¿‡ Attention Mask å¯ä»¥çœ‹åˆ°æ‰€æœ‰çš„æ¦‚å¿µå‰ç¼€ï¼Œä½†æ¦‚å¿µå‰ç¼€å†…éƒ¨æ˜¯åŸºäº Causal Mask æˆ–ç‰¹å®šå¯è§æ€§è®¾ç½®çš„ã€‚
    *   **Loss**: æ ‡å‡†çš„ CrossEntropyLossï¼Œè¡¡é‡åœ¨ç»™å®šæ¦‚å¿µè§„åˆ’ä¸‹æ¢å¤åŸæ–‡æœ¬çš„èƒ½åŠ›ã€‚

##### 3. æ•°æ®æµæ€»ç»“

1.  **Input Text** -> **Backbone** -> **<PLAN>** -> **Planner Generation (Gumbel Softmax)** -> **Concept Tokens**.
2.  **Concept Tokens (ä½œä¸º Prefix)** + **Input Text** -> **Backbone** -> **Text Reconstruction Loss**.

##### å…³é”®ç‚¹æ€»ç»“
*   **ç«¯åˆ°ç«¯å¯å¾®**ï¼šé€šè¿‡ Gumbel-Softmax æˆ–ç±»ä¼¼çš„ç›´é€šä¼°è®¡ï¼ˆStraight-Through Estimatorï¼‰ï¼Œè®©æ‰§è¡Œå™¨çš„ Loss å¯ä»¥æ›´æ–°è§„åˆ’å™¨çš„å‚æ•°ï¼Œä½¿æ¨¡å‹å­¦ä¼šç”Ÿæˆâ€œæœ‰åˆ©äºè¿˜åŸæ–‡æœ¬â€çš„æ¦‚å¿µã€‚
*   **å¤šç±»å‹æ¦‚å¿µ**ï¼šæ”¯æŒå¤šç§ä¸åŒå±æ€§çš„æ¦‚å¿µï¼ˆç”± `metas` å®šä¹‰ï¼‰ï¼Œæ¨¡å‹ä¼šæŒ‰é¡ºåºæˆ–å¹¶è¡Œè§„åˆ’è¿™äº›ä¸åŒå±‚çº§çš„å‹ç¼©è¡¨ç¤ºã€‚
*   **LoRA å¾®è°ƒ**ï¼šä¿è¯äº†è®­ç»ƒæ•ˆç‡ï¼Œä¸»è¦è°ƒæ•´çš„æ˜¯ LoRA å‚æ•°å’Œæ–°å¢çš„ Head/Embedding å‚æ•°ã€‚

è¿™ä¸ªæ¶æ„éå¸¸é€‚åˆ**æ–‡æœ¬å‹ç¼©**ã€**å—æ§ç”Ÿæˆ**æˆ–**é•¿æ–‡æœ¬å»ºæ¨¡**ï¼ˆé€šè¿‡é«˜å±‚æ¦‚å¿µå‡å°‘åºåˆ—é•¿åº¦ï¼‰ä»»åŠ¡ã€‚æˆ‘å·²å®Œå…¨ç†è§£è¯¥æ¶æ„ã€‚

## v3.1: speed up training
1. é¢„æµ‹concept tokensæ—¶MTP ï¼ˆæœ‰ç‚¹ç±»ä¼¼äºARç»“åˆqueryï¼‰
2. é¢„æµ‹ä¸åŒtypeçš„concept tokensæ—¶å¹¶è¡Œè·‘
3. zæ¯éš”Mæ­¥æ›´æ–°ä¸€æ¬¡
4. å…ˆç”¨çŸ­æ–‡æœ¬è®­ç¨³ï¼Œåç»­å†ç”¨é•¿æ–‡æœ¬

## ä½ å¯ä»¥åšä»€ä¹ˆå®éªŒ
1. éšæœºå»æ‰ä¸€äº›concept tokensï¼Œçœ‹çœ‹é‡å»ºæ•ˆæœæ˜¯å¦ä¼šå—å¾ˆå¤§å½±å“ï¼Œä»è€Œè¯å®executoræ˜¯çœŸçš„åœ¨ç”¨concept tokens

ä¸‹é¢è¿™äº›æŒ‡æ ‡éƒ½**å¾ˆä¾¿å®œ**ï¼ˆåŸºæœ¬æ¥è‡ªå·²æœ‰ forward çš„ logits / loss / beam ç»“æœï¼‰ï¼Œä½†èƒ½å¾ˆå¥½åœ°åˆ¤æ–­ï¼šE-step æœç´¢æœ‰æ²¡æœ‰è·‘åã€planner æœ‰æ²¡æœ‰å­¦åˆ°ä¸œè¥¿ã€ä»¥åŠä½ è¿™ä¸ª STï¼ˆ(e_{hard}+e_{soft}-\text{stopgrad}(e_{soft}))ï¼‰æœ‰æ²¡æœ‰åœ¨â€œéª—æ¢¯åº¦â€ã€‚

æˆ‘æŒ‰æ¨¡å—ç»™ä¸€å¥—æœ€å®ç”¨çš„ç›‘æ§æ¸…å•ï¼Œå¹¶æ ‡æ³¨â€œæ­£å¸¸/å¼‚å¸¸â€ä¿¡å·ã€‚

---
----
----
----
## A. Executor æ˜¯å¦çœŸçš„åœ¨ç”¨ zï¼ˆæœ€å…³é”®ï¼‰

### 1) æ¡ä»¶äº’ä¿¡æ¯ä»£ç†ï¼šÎ”NLLï¼ˆablation å·®ï¼‰

æ¯éš”ä¸€æ®µï¼ˆæ¯”å¦‚æ¯ 500ï½2000 stepï¼‰ï¼Œåœ¨ä¸€ä¸ªå¾ˆå°çš„ mini-batch ä¸Šåšä¸¤æ¬¡ teacher forcingï¼š

* æ­£å¸¸ï¼šç”¨å½“å‰é€‰çš„ (z^*) è®¡ç®— ( \text{NLL}(y\mid z^*))
* ç ´å zï¼šæŠŠ z æ¢æˆ

  * åŒ batch é‡Œåˆ«çš„æ ·æœ¬çš„ zï¼ˆshuffle zï¼‰ï¼Œæˆ–
  * å…¨ <MASK>/<PAD>ï¼Œæˆ–
  * éšæœº zï¼ˆåŒé•¿åº¦ï¼‰

è®°ï¼š
[
\Delta = \text{NLL}(y\mid z_{\text{bad}})-\text{NLL}(y\mid z^*)
]

**æ­£å¸¸**ï¼šÎ” > 0 ä¸”é€æ­¥å˜å¤§/ç¨³å®šï¼ˆè¯´æ˜ z æœ‰ä¿¡æ¯ï¼‰
**å¼‚å¸¸**ï¼šÎ”â‰ˆ0ï¼ˆexecutor å¿½ç•¥ zï¼›posterior collapse é£é™©ï¼‰

> è¿™ä¸æ˜¯â€œå¤§é‡è®¡ç®—â€ï¼šåªæ˜¯åœ¨å° batch ä¸Šå¤šè·‘ä¸€æ¬¡ forwardã€‚

---

## B. E-step æœç´¢è´¨é‡ä¸ç¨³å®šæ€§ï¼ˆHard-EM æ˜¯å¦å¥åº·ï¼‰

### 2) Top-1 marginï¼ˆèƒœå‡ºå¹…åº¦ï¼‰

ä½ åœ¨ E-step ä¼šå¾—åˆ° top-k å€™é€‰çš„è¯„åˆ† (S(z)=\log p_\theta(y|z)+\log p_\phi(z|u))ã€‚

ç›‘æ§ï¼š
[
m = S(z^{(1)})-S(z^{(2)})
]

**æ­£å¸¸**ï¼šm é€‚ä¸­ï¼ˆæœ‰åŒºåˆ†åº¦ï¼Œä½†ä¸æ€»æ˜¯å·¨å¤§ï¼‰
**å¼‚å¸¸1**ï¼šmâ‰ˆ0 é•¿æœŸï¼ˆæœç´¢ä¸åˆ†èƒœè´Ÿï¼Œå™ªå£°å¤§ï¼Œz* ä¸ç¨³å®šï¼‰
**å¼‚å¸¸2**ï¼šm å·¨å¤§ä¸”å¾ˆæ—©å‡ºç°ï¼ˆè¿‡æ—©å¡Œç¼©åˆ°å›ºå®šæ¨¡å¼/ä½å¤šæ ·æ€§ï¼‰

### 3) z* å˜æ›´ç‡ï¼ˆassignment flip rateï¼‰

æ¯æ¬¡ä½ æ›´æ–° E-step æ—¶ï¼Œç»Ÿè®¡ï¼š

* æœ‰å¤šå°‘æ ·æœ¬çš„ (z^*) è·Ÿä¸Šæ¬¡ä¸åŒï¼ˆæˆ–ç¼–è¾‘è·ç¦»>é˜ˆå€¼ï¼‰

**æ­£å¸¸**ï¼šå‰æœŸé«˜ã€åæœŸä¸‹é™
**å¼‚å¸¸**ï¼šä¸€ç›´å¾ˆé«˜ï¼ˆæ¨¡å‹åœ¨éœ‡è¡/å­¦ä¹ ç‡è¿‡å¤§/è¯„åˆ†ä¸å¯é ï¼‰
**å¼‚å¸¸**ï¼šå¾ˆæ—©é™åˆ°æ¥è¿‘ 0ï¼ˆè¿‡æ—©é”æ­»ï¼Œå¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰

> è¿™ä¸ªå‡ ä¹é›¶æˆæœ¬ï¼šä½ æœ¬æ¥å°±æœ‰ z*ï¼Œåªè¦åšä¸ª hash/ç¼–è¾‘è·ç¦»ã€‚

### 4) â€œæ—§ z* ä»å¥½ä¸å¥½ç”¨â€ï¼ˆstaleness gapï¼‰

åœ¨ç¨€ç–æ›´æ–° E-step æ—¶ï¼š
åŒä¸€ä¸ªæ ·æœ¬ï¼Œç”¨æ—§ (z^**{\text{old}}) å’Œæ–° (z^**{\text{new}}) çš„åˆ†æ•°å·®ï¼š

[
g = S(z^**{\text{new}})-S(z^**{\text{old}})
]

**æ­£å¸¸**ï¼šg å°ä¸”é€æ­¥å˜å°ï¼ˆè¯´æ˜ç¨€ç–æ›´æ–°æ²¡ä¼¤å¤ªå¤šï¼‰
**å¼‚å¸¸**ï¼šg å¸¸å¸¸å¾ˆå¤§ï¼ˆè¯´æ˜ä½  stale å¤ªä¹…ï¼Œåº”è¯¥æ›´é¢‘ç¹æ›´æ–°æˆ–å‡å°LRï¼‰

---

## C. Planner æ˜¯å¦åœ¨å­¦ï¼ˆä»¥åŠ ST æ˜¯å¦â€œå¯¹é½â€ï¼‰

### 5) Planner ç†µï¼ˆæˆ– top-1 æ¦‚ç‡å‡å€¼ï¼‰

ä» planner çš„ soft åˆ†å¸ƒé‡Œå–ä¸€ä¸ªä¾¿å®œç»Ÿè®¡ï¼š

* æ¯ä¸ªä½ç½®çš„ entropy (H(p_t)) çš„å‡å€¼
* æˆ– mean max-probï¼š(\mathbb{E}[\max_i p_t(i)])

**æ­£å¸¸**ï¼šå‰æœŸé«˜ç†µâ†’ä¸­æœŸé€æ­¥å˜å°–â†’åæœŸç¨³å®š
**å¼‚å¸¸1**ï¼šä¸€ç›´é«˜ç†µï¼ˆplanner æ²¡å­¦åˆ°å¯ç”¨ latentï¼‰
**å¼‚å¸¸2**ï¼šå¾ˆå¿«æä½ç†µï¼ˆè¿‡æ—©ç¡®å®šã€å€™é€‰å¤šæ ·æ€§ä¸è¶³ã€Hard-EM æ˜“å¡æ­»ï¼‰

### 6) e_soft ä¸ e_hard çš„ä¸€è‡´æ€§ï¼ˆcosine / L2ï¼‰

ä½ æœ¬æ¥å°±ç®—äº† (e_{soft}) å’Œ (e_{hard})ã€‚ç›‘æ§ï¼š

* (\cos(e_{soft}, e_{hard})) çš„å‡å€¼ï¼ˆæŒ‰ token æˆ–æŒ‰åºåˆ—å¹³å‡ï¼‰
* æˆ– (|e_{soft}-e_{hard}|)

**æ­£å¸¸**ï¼šéšè®­ç»ƒé€æ­¥æ›´ä¸€è‡´ï¼ˆsoft æ›´æ¥è¿‘ one-hotï¼‰
**å¼‚å¸¸**ï¼šé•¿æœŸä¸ä¸€è‡´ï¼ˆST æ¢¯åº¦åœ¨ä¼˜åŒ–â€œæ¨¡ç³Š embeddingâ€ï¼Œforward å´èµ°ç¡¬ tokenï¼Œå®¹æ˜“ driftï¼‰

> è¿™ä¸ªä¹Ÿå‡ ä¹é›¶æˆæœ¬ï¼Œå› ä¸º embedding å·²ç»åœ¨ forward é‡Œäº†ã€‚

### 7) Planner æ¢¯åº¦èŒƒæ•°ï¼ˆå…¨å±€æˆ–æœ€åä¸€å±‚ï¼‰

è®°å½•ä¸€ä¸ªç®€å•æ ‡é‡ï¼š

* (|\nabla_\phi|)ï¼ˆglobal grad normï¼‰
* æˆ–æŸå±‚ grad norm

**æ­£å¸¸**ï¼šæœ‰ä¿¡å·ã€ä¸è¿‡åº¦çˆ†ç‚¸
**å¼‚å¸¸**ï¼šæ¥è¿‘ 0ï¼ˆexecutor ä¸ä¾èµ– z / ST æ— æœ‰æ•ˆä¿¡å·ï¼‰
**å¼‚å¸¸**ï¼šé¢‘ç¹çˆ†ç‚¸ï¼ˆæ¸©åº¦è¿‡ä½ã€å­¦ä¹ ç‡è¿‡å¤§ã€è¯„åˆ†éœ‡è¡ï¼‰

---

## D. è¾“å‡ºä¾§ï¼šè®­ç»ƒæ˜¯å¦åœ¨â€œé•¿åº¦æ‰©å±•â€æ—¶å´©

### 8) åˆ†æ®µ NLLï¼ˆæŒ‰ä½ç½®æ¡¶ï¼‰

ä¸éœ€è¦é¢å¤– forwardï¼šä½  teacher forcing æœ¬æ¥å°±ç®—äº†æ¯ä¸ª token çš„ lossã€‚
æŠŠ loss æŒ‰ä½ç½®åˆ†æ¡¶ï¼ˆæ¯”å¦‚ 0-64, 64-128, â€¦ï¼‰æ±‚å‡å€¼ã€‚

**æ­£å¸¸**ï¼šå„æ¡¶éƒ½é€æ­¥ä¸‹é™ï¼›æ‰©å±•é•¿åº¦æ—¶åæ®µå…ˆæŠ–å†ç¨³
**å¼‚å¸¸**ï¼šåªå‰ 64 é™ï¼Œåé¢ä¸€ç›´ä¸åŠ¨ï¼ˆz å¯èƒ½åªæœåŠ¡å¼€å¤´ / executor é•¿ç¨‹æ²¡å­¦åˆ°ï¼‰

---

# ä¸€å¥—æœ€å°ä½†å¤Ÿç”¨çš„ç›‘æ§ç»„åˆï¼ˆå»ºè®®ä½ å°±ç”¨è¿™ 6 ä¸ªï¼‰

1. **Î”NLL(z-shuffle)**ï¼šexecutor æ˜¯å¦ç”¨ z
2. **Top-1 margin**ï¼šE-step åŒºåˆ†åº¦
3. **z* flip rate**ï¼šç¨³å®šæ€§/æ˜¯å¦éœ‡è¡
4. **staleness gap**ï¼šç¨€ç–æ›´æ–°æ˜¯å¦è¿‡å¤´
5. **planner entropy / max-prob**ï¼šæ˜¯å¦å¡Œç¼©æˆ–æ²¡å­¦
6. **cos(e_soft, e_hard)**ï¼šST å¯¹é½æ˜¯å¦å¥åº·

è¿™äº›å‡ ä¹éƒ½ä¸éœ€è¦é¢å¤–å¤§è®¡ç®—ï¼Œåªæœ‰ç¬¬1ä¸ªéœ€è¦å¶å°”å¤šä¸€æ¬¡ forwardï¼ˆå° batch å³å¯ï¼‰ã€‚

---

## å¿«é€Ÿâ€œå¼‚å¸¸â†’åŠ¨ä½œâ€å¯¹ç…§è¡¨

* Î”NLLâ‰ˆ0ï¼šåŠ å¼º bottleneckï¼ˆexecutor ä¸çœ‹ uï¼‰ã€å¢å¤§ z å®¹é‡/è¯è¡¨ã€é™ä½ executor å­¦ä¹ ç‡ã€æé«˜ z dropout/æ‰°åŠ¨å¯¹æ¯”
* marginâ‰ˆ0 ä¸” flip é«˜ï¼šæé«˜å€™é€‰ K æˆ–å¤šæ ·æ€§ï¼›E-step æ›´é¢‘ç¹ï¼›é™ä½å­¦ä¹ ç‡
* flip å¾ˆæ—©å˜ 0 + ç†µå¾ˆä½ï¼šæé«˜æ¸©åº¦/ç†µæ­£åˆ™ï¼›ç”¨ diverse beamï¼›å¶å°”å¼ºåˆ¶æ¢ç´¢ï¼ˆé‡‡æ ·å€™é€‰ï¼‰
* staleness gap å¤§ï¼šç¼©çŸ­ E-step æ›´æ–°é—´éš”æˆ–ç”¨è‡ªé€‚åº”æ›´æ–°ï¼›å‡ LR
* cos(e_soft,e_hard) é•¿æœŸä½ï¼šé™æ¸©åº¦æˆ– annealï¼›å¯¹ soft åŠ  sharpenï¼ˆä½†åˆ«ä¸€ä¸‹å­å¤ªç¡¬ï¼‰
