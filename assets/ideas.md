## conditions
### must-have
- high efficiency
- autoregressive training/inference
- discrete concept tokens
- minimal structural changes to the base LLM (LoRA + small new modules)

### preferred
- internal concept tokens only (not exposed to user text)
- online compression during decoding

## v1
- Kept as baseline in `train_qwen.py`.

## v2: Online Concept Compression with Concept-Only Middle Blocks

### core design
- Split decoder layers into:
  - shallow blocks: process all normal tokens
  - middle blocks: process concept tokens only
  - deep blocks: process concept prefix + dynamic normal tail
- Add a `ConceptHead` on top of shallow hidden states:
  - output size = `4096 + 1` (`4096` concept ids + `<NULL>`)
  - use Gumbel-Softmax with straight-through estimator
  - `<NULL>` means current normal token is not represented as concept token

### online boundary
- For each time step `t`, define:
  - `tau(t)`: most recent compressed normal token position (`!= <NULL>`)
  - `tail(t) = h_shallow[tau(t)+1 : t]`
- Deep input at step `t`:
  - `Z_t = [concept_prefix(<=t); tail(t)]`

### deep attention rules
- Concept -> Concept: allowed
- Tail -> Concept: allowed
- Tail -> Tail: causal
- Concept -> Tail: disallowed

### training objective (current version)
- Pure next-token cross entropy only:
  - `CE(logits_t, x[t+1])`
- No extra auxiliary losses and no additional training tricks in this version.

### trainable parts
- New modules:
  - `ConceptHead`
  - `ConceptEmbedding`
- Base model:
  - LoRA adapters on attention/MLP linear layers
- Base backbone weights are frozen except LoRA parameters.

### implementation file
- `train_qwen_2.py`


### å®ç°çš„tricks
1. æ§åˆ¶NULL tokensçš„æ•°é‡
2. æ§åˆ¶è¿ç»­çš„NULL tokensçš„æ•°é‡
   
### ä¼˜åŠ¿
1. ç›¸æ¯”äºLLM as compressor and decompressorï¼Œå¯ä»¥æŠŠç”Ÿæˆçš„å†…å®¹ä¹Ÿå‹ç¼©æˆlatent tokensï¼Œè¿™æ ·ä¸concept tokensçš„æ•…äº‹å¯èƒ½èåˆå¾—æ›´å¥½
2. E2E

---


---

### prompt
å¥½ï¼Œé‚£æˆ‘ç›´æ¥ç»™ä½ ä¸€ä»½**â€œæç®€ + ä¼ªä»£ç çº§â€çš„ coding agent prompt**ï¼ŒåŸºæœ¬æ˜¯ä½ è„‘å­é‡Œé‚£å¥—é€»è¾‘çš„**å¯æ‰§è¡Œè§„æ ¼**ï¼Œå°½é‡å‡å°‘è‡ªç”±å‘æŒ¥ç©ºé—´ã€‚

---

#### Ultra-Compact Coding Agent Promptï¼ˆPseudo-code Styleï¼‰

**Task**
Implement training code for an autoregressive LM with **online concept compression** and **concept-only middle Transformer blocks**.

---

##### Core Idea

* Normal tokens are processed by **shallow layers**.
* After shallow layers, a **concept head** compresses some tokens into **concept tokens** using **Gumbel-Softmax + straight-through**.
* **Middle Transformer blocks run ONLY on concept tokens** (main compute saving).
* **Deep Transformer blocks** see:

  * all concept tokens
  * * a dynamic tail of normal tokens (from last compressed token + 1 to current token)
* Model predicts next normal token autoregressively.

---

##### Modules

```text
ShallowTransformer(x[1:T]) -> h_L[1:T]

ConceptHead(h_L[t]):
    logits = linear(h_L[t])
    z = gumbel_softmax(logits, hard=True)
    e = e_hard + (e_soft - stop_grad(e_soft))
    if argmax(z) == <NULL>: return None
    else: return concept_token
```

```text
MiddleTransformer(concept_tokens) -> concept_tokens_updated
# ONLY concept tokens go through these layers
```

```text
DeepTransformer([concept_tokens_updated ; tail_hiddens]) -> logits
```

---

##### Boundary / Tail Definition

```text
Ï„(t) = index of most recent normal token whose concept != <NULL>
tail(t) = h_L[Ï„(t)+1 : t]    # contiguous, variable length
```

---

##### Training Forward (single sequence)

```text
h_L = ShallowTransformer(x[1:T])

concept_tokens = []
compressed_pos = set()

for t in 1..T:
    c = ConceptHead(h_L[t])
    if c != None:
        concept_tokens.append(c)
        compressed_pos.add(t)

concept_tokens_mid = MiddleTransformer(concept_tokens)

for t in 1..T-1:
    Ï„ = max{i <= t | i in compressed_pos} or 0
    tail = h_L[Ï„+1 : t]

    Z_t = concat(concept_tokens_mid_prefix(t), tail)
    logits_t = DeepTransformer(Z_t)
    loss += CE(logits_t, x[t+1])
```

---

##### Attention Rules (DeepTransformer)

```text
Concept -> Concept : allowed (causal or full)
Tail    -> Concept : allowed
Tail    -> Tail    : causal
Concept -> Tail    : disallowed
```

---

##### Key Constraints

* Middle blocks never see normal tokens.
* `<NULL>` means â€œthis token is NOT represented in deep contextâ€.
* Concept tokens must replace information of compressed normal tokens.
* End-to-end backprop through:
  shallow â†’ concept head (ST) â†’ middle â†’ deep.

---

å¦‚æœä½ ä¸‹ä¸€æ­¥è¦**çœŸæ­£å†™ PyTorch ä»£ç **ï¼Œæˆ‘å¯ä»¥ç»§ç»­å¸®ä½ åšä¸‰ä»¶äº‹ä¹‹ä¸€ï¼ˆä½ é€‰ä¸€ä¸ªå°±è¡Œï¼‰ï¼š

1. æŠŠä¸Šé¢ä¼ªä»£ç  **ç¿»æˆæœ€å° PyTorch ç±»ç»“æ„**
2. ä¸“é—¨å†™ **attention mask / padding é€»è¾‘**ï¼ˆè¿™æ˜¯æœ€å®¹æ˜“å†™é”™çš„ï¼‰
3. è®¾è®¡ä¸€ä¸ª **O(T) è€Œä¸æ˜¯ O(TÂ²)** çš„ batching / forward å®ç°æ–¹å¼


## ç©¶ç«Ÿæ€æ ·å˜é•¿ï¼Ÿ
1. ç”¨æ•´ä¸ªLLM çº¯è‡ªå›å½’ 
2. NULL tokens
3. åŠ å’Œ + é˜ˆå€¼
4. å›ºå®šK -> æ¯Nä¸ªæ™®é€štokensç”ŸæˆMä¸ªlatent tokens
5. å…ˆMTPç”Ÿæˆè‹¥å¹²æ™®é€štokensï¼Œmaybeå†ç”¨ä¸€äº›å¯å‘å¼æ–¹æ³•ç»ˆæ­¢ï¼ˆæ¯”å¦‚é«˜ç†µçš„tokensï¼‰

## v3: pure AR
* ä¼˜åŠ¿ï¼š
  * å‚æ•°æ›´å°‘
  * å†…å­˜å ç”¨æ›´å°‘
  * æ›´å¿«ï¼ˆä¸éœ€è¦prefillå¤šæ¬¡ï¼‰

* how to make it more non-trivial?
  * 1. 2d-RoPE Ã— ï¼ˆè¯­ä¹‰æ ¹æœ¬å°±ä¸å¯¹ï¼‰--> éƒ½ä»1å¼€å§‹è®¡æ•°ï¼Œä¸åŒç§ç±»çš„tokensåŠ ä¸€ä¸ªå›ºå®šbias
  * 2. å¤šå±‚æ¬¡concept (å¯ç»“åˆ2d-RoPE)
  * 3. ä»ç›®æ ‡æ–‡æœ¬ X è‡ªåŠ¨æŠ½å–å¯ç›‘ç£ä¿¡å· ğ‘¦(ğ‘‹)ï¼Œè®© Planner å¿…é¡»èƒ½ä» C é¢„æµ‹è¿™äº› y

----

å¥½ï¼Œä¸‹é¢æ˜¯**æŒ‰ä½ æœ€æ–°ä¿®æ”¹**åçš„**ç®€æ´ä¸­è‹±åŒè¯­ pipeline æ€»ç»“**ï¼Œå¯ä»¥ç›´æ¥ä½œä¸ºç»™ codex çš„æç¤ºè¯ä½¿ç”¨ï¼ˆå·²æ»¡è¶³ï¼šä¸åŒ concept ç±»å‹â†’ä¸åŒè¯è¡¨ï¼›ç»Ÿä¸€ RoPEï¼›ä¸æ segment_posï¼‰ã€‚

---
### prompt
#### ä¸­æ–‡ï¼ˆç®€æ´æç¤ºè¯ï¼‰

ç›®æ ‡ï¼šPlannerâ€“Executor ä¸¤é˜¶æ®µ **Concept-first Decoding**ã€‚concept tokens æ˜¯å†…éƒ¨é«˜å±‚è®¡åˆ’ï¼ŒExecutor ä»…ä¾èµ– concept ç”Ÿæˆæ™®é€šæ–‡æœ¬ã€‚

**Pipelineï¼š**

1. **Plannerï¼ˆæ¦‚å¿µè§„åˆ’ï¼‰**

   * è¾“å…¥ï¼šç”¨æˆ·æŒ‡ä»¤/ä¸Šä¸‹æ–‡ (u)
   * è¾“å‡ºï¼šå¤šç§ç±» concept tokensï¼Œæ¯ä¸€ç±»æ¥è‡ª**ç‹¬ç«‹è¯è¡¨**ï¼š
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * æ¯ä¸€ç±» concept ä¸ºå˜é•¿åºåˆ—ï¼Œä»¥ `<EOS>` ç»“æŸã€‚

2. **æ„é€  Executor è¾“å…¥ï¼ˆä»… conceptï¼‰**

   * å°†ä¸åŒç§ç±»çš„ concept ä¸²è”ï¼Œä¸­é—´ç”¨å¯¹åº”çš„åˆ†éš”ç¬¦ï¼š
     `[<BOS>, <SEP_1>, C^{(1)}, <SEP_2>, C^{(2)}, ...]`
   * **ä½ç½®ç¼–ç **ï¼š

     * æ¯ç§ concept token çš„ position index **å„è‡ªä» 1 å¼€å§‹è®¡æ•°**
     * æ‰€æœ‰ token **ç»Ÿä¸€ä½¿ç”¨ RoPE**
   * **Type embedding**ï¼š

     * ä¸ºæ¯ç§ concept ç±»å‹ï¼ˆä»¥åŠå¯¹åº”çš„ `<SEP>`ï¼‰åŠ å…¥ type embedding
   * æœ€ç»ˆè¾“å…¥ embeddingï¼š
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executorï¼ˆå±•å¼€ç”Ÿæˆï¼‰**

   * æ¡ä»¶ç”Ÿæˆï¼š
     [
     x_{1:T} \sim p(x \mid C)
     ]
   * Executor **ä¸è¯»å–æ™®é€š tokens å‰ç¼€ä½œä¸ºæ¡ä»¶ä¸Šä¸‹æ–‡**
   * æ™®é€š token æŒ‰æ ‡å‡†è‡ªå›å½’ç”Ÿæˆï¼Œæ”¯æŒ KV cacheã€‚

---

#### English (concise prompt)

Goal: Two-stage **Plannerâ€“Executor Concept-first Decoding**. Concept tokens are internal high-level plans; the Executor generates text conditioned only on concepts.

**Pipeline:**

1. **Planner (Concept planning)**

   * Input: user instruction/context (u)
   * Output: multiple **typed concept sequences**, each from a **separate vocabulary**:
     [
     C = {C^{(1)}, C^{(2)}, \dots}, \quad C^{(i)} \subset V_c^{(i)}
     ]
   * Each concept sequence is variable-length and ends with `<EOS>`.

2. **Build Executor input (concept-only)**

   * Concatenate concept sequences with type-specific separators:
     `[<BOS>, <SEP_1>, C^(1), <SEP_2>, C^(2), ...]`
   * **Positional encoding**:

     * Position indices **restart from 1 for each concept type**
     * **Unified RoPE** is applied to all tokens
   * **Type embedding**:

     * Add a learned type embedding for each concept type (and its separator)
   * Final embedding:
     `tok_emb + RoPE(pos) + type_emb(type_id)`

3. **Executor (Expansion / generation)**

   * Generate text as:
     [
     x_{1:T} \sim p(x \mid C)
     ]
   * The Executor conditions **only on concept tokens** (no normal-token prefix as input).
   * Standard autoregressive decoding with KV cache.


